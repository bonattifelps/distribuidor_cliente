{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pyspark.sql import SparkSession, DataFrame, functions as F \nfrom pyspark.sql.window import Window \nfrom pyspark import StorageLevel \nfrom datetime import datetime \nimport logging \nfrom typing import Tuple \nfrom dataclasses import dataclass\n\n@dataclass\nclass ConfigDistribuicao:\n    \"\"\"Configurações do processo de distribuição\"\"\"\n    limite_clientes: int = 1473\n\ndef setup_logging() -> logging.Logger:\n    \"\"\"Configura o logger\"\"\"\n    logger = logging.getLogger(\"distribuicao\")\n    if not logger.handlers:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n    return logger\n\ndef get_spark_session(config: ConfigDistribuicao) -> SparkSession:\n    \"\"\"Inicializa a SparkSession\"\"\"\n    return (SparkSession.builder\n        .appName(\"DistribuicaoClientes\")\n        .config(\"spark.sql.shuffle.partitions\", \"600\")\n        .getOrCreate())\n\ndef load_data(spark: SparkSession, logger: logging.Logger) -> Tuple[DataFrame, DataFrame]:\n    \"\"\"Carrega dados de clientes e gerentes\"\"\"\n    logger.info(\"Carregando dados de clientes e gerentes...\")\n   \n    # Carrega os dados\n    clientes_df = spark.table(\"path.base_clientes\")\n    gerentes_df = spark.table(\"path.base_gerentes\")\n\n    # Verificação inicial dos dados\n    logger.info(f\"Total de clientes carregados: {clientes_df.count()}\")\n\n    clientes_df.show(10, truncate=False)\n    gerentes_df.show(10, truncate=False)\n\n    logger.info(f\"Total de gerentes carregados: {gerentes_df.count()}\")\n\n    # Ajustar filtro para incluir apenas registros válidos\n    clientes_df = clientes_df.filter(F.col(\"chave_unica\").isNotNull())\n    gerentes_df = gerentes_df.filter(F.col(\"chave_unica\").isNotNull())\n\n    # Verificação após filtro\n    logger.info(f\"Total de clientes após filtro: {clientes_df.count()}\")\n    logger.info(f\"Total de gerentes após filtro: {gerentes_df.count()}\")\n\n    return clientes_df, gerentes_df\n\ndef distribuir_clientes(clientes_df: DataFrame, gerentes_df: DataFrame, config: ConfigDistribuicao, logger: logging.Logger) -> DataFrame:\n    \"\"\"Realiza a distribuição de clientes para gerentes\"\"\"\n    logger.info(\"Iniciando processo de distribuição...\")\n    \n    # Faz a combinação entre clientes e gerentes com a mesma chave `chave_unica`\n    combinacao = clientes_df.join(gerentes_df, \"chave_unica\")\n    \n    # Define uma janela para balancear a distribuição entre os gerentes dentro de cada `chave_unica`\n    window_dist = Window.partitionBy(\"nr_cli\").orderBy(F.rand())\n    \n    # Atribui um índice de linha para cada combinação de cliente e gerente\n    distribuicao = combinacao.withColumn(\n        \"row_number\",\n        F.row_number().over(window_dist)\n    ).filter(F.col(\"row_number\") == 1).drop(\"row_number\")\n\n    # Verifica se todos os clientes foram distribuídos\n    clientes_distribuidos = distribuicao.select(\"nr_cli\").distinct().count()\n    total_clientes = clientes_df.select(\"nr_cli\").distinct().count()\n\n    if clientes_distribuidos != total_clientes:\n        raise Exception(\"Erro: Nem todos os clientes foram distribuídos!\")\n\n    logger.info(f\"Distribuição concluída: {distribuicao.count()} clientes atribuídos.\")\n    return distribuicao\n\ndef validar_distribuicao(df: DataFrame, logger: logging.Logger) -> None:\n    \"\"\"Valida a distribuição realizada\"\"\"\n    logger.info(\"Validando a distribuição...\")\n    duplicados = df.groupBy(\"nr_cli\").agg(F.count(\"*\").alias(\"qtd\")).filter(F.col(\"qtd\") > 1)\n    duplicados_count = duplicados.count()\n \n    if duplicados_count > 0:\n        logger.error(f\"Encontrados {duplicados_count} CPFs duplicados na distribuição.\")\n        raise Exception(\"Erro: Clientes atribuídos a mais de um gerente!\")\n \n    logger.info(\"Validação concluída: cada cliente foi atribuído a um único gerente.\")\n\n\ndef salvar_resultado(df: DataFrame, logger: logging.Logger) -> None:\n    \"\"\"Salva os resultados da distribuição\"\"\"\n    logger.info(\"Salvando os resultados da distribuição...\")\n    \n    # Selecionar os campos desejados\n    tabela_final = df.select(\"CAMPO1\",\"CAMPO2\"...,\"CAMPO-N\")\n    \n    # Salvar os resultados\n    tabela_final.write.saveAsTable('path.prenome', mode='overwrite', overwriteSchema='true', mergeSchema='true')\n    logger.info(\"Resultados salvos com sucesso.\")\n\n\ndef main():\n    config = ConfigDistribuicao()\n    spark = get_spark_session(config)\n    logger = setup_logging()\n \n    logger.info(\"=== Início do processo de distribuição ===\")\n    try:\n        # Carregamento dos dados\n        clientes_df, gerentes_df = load_data(spark, logger)\n \n        # Distribuição de clientes\n        resultado_df = distribuir_clientes(clientes_df, gerentes_df, config, logger)\n \n        # Validação dos resultados\n        validar_distribuicao(resultado_df, logger)\n \n        # Salvamento dos resultados\n        salvar_resultado(resultado_df, logger)\n \n        logger.info(\"=== Processo concluído com sucesso ===\")\n \n    except Exception as e:\n        logger.error(f\"Erro crítico: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n    \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DECLARE \n  MES_REF VARCHAR2(6);\n  ANOMES VARCHAR2(6);\nBEGIN\n  \nANOMES := '202311'; \n\nFOR i IN 0..14\n  LOOP\n\n      MES_REF := TO_CHAR(ADD_MONTHS(TO_DATE(DATA, 'RRRRMM'),-i), 'RRRRMM');\n      \n      EXECUTE IMMEDIATE('\n            DELETE FROM SCHEMA.TABLE\n            WHERE CHAVE_DATA(A) = TO_CHAR(ADD_MONTHS(TO_DATE('||MES_REF||', ''RRRRMM''),- TO_NUMBER(REPLACE(SAFRA,''M'','''')) ), ''RRRRMM'')    \n                  AND SAFRA = ''M''||MONTHS_BETWEEN(TO_DATE('||MES_REF||',''RRRRMM''), TO_DATE(CHAVE_DATA, ''RRRRMM''))\n      ');      \n\n      EXECUTE IMMEDIATE('\n      INSERT INTO SCHEMA.TABLE\n             SELECT /*+PARALLEL(16)*/\n                 A.CAMPO_EXEMPLO,\n                 ''M''||MONTHS_BETWEEN(TO_DATE(NVL(B.CHAVE_DATA, '||MES_REF||'),''RRRRMM''), TO_DATE(A.CHAVE_DATA, ''RRRRMM'') ) AS SAFRA\n\n\n\n\n\nFROM (SELECT * FROM PATH.TABELA_PRINCIPAL) A\n\n          LEFT JOIN PATH.TABELA_JOIN PARTITION(P_'||MES_REF||') B\n          ON A.CHAVE_A= B.CHAVE_B\n             AND A.CHAVE2_A = B.CHAVE2_B\n             AND A.CHAVE_DATA <= B.CHAVE_DATA","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%python\nimport pymsteams\nimport pandas as pd\nimport pyspark.sql.functions as F\nimport pyspark.sql as S\nimport requests\n\ndf = sqlContext.sql(\"\"\"\nselect codigo_x AS QTD_CLI,\n date_format(atualizacao,'yyyyMM' )  AS DATA_REFERENCIA\n from --PATH.SUA_TABLE\nWHERE xpto1 = 'F' \nAND xpto2 between '2024-06-01' AND '2024-06-31' \nAND xpto3 IN ('004', '005', '006', '007', '106','001', '002', '003', '304','009')\nAND xpto4 IN ('N')\nAND atualizacao = '2024-06-28'\n\"\"\")\n\nif (df.count() > 0):\n    texto = \"MONITORAMENTO XPTO - CLOUD \\n\"\n    texto = texto + \"\\n\"\n\n    if (df.count() < 488000):\n      texto = texto + \"TEXTO \" + str(df.count()) + \" TEXTO \\n\"\n    else:\n      texto = texto + \"TEXTO \" + str(df.count()) + \" TEXTO_ERRO. Verificar a query. \\n\"\n\n    print(texto)\n\n    \nmyTeamsMessage = pymsteams.connectorcard(\"conector webhook\")\nmyTeamsMessage.text(texto)\nmyTeamsMessage.send()\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType, TimestampType\n\nclass OracleClient:\n    def __init__(self, host, port, sid, user, password):\n        \"\"\"\n        Inicializa a conexão JDBC com o banco Oracle.\n        \n        :param host: Endereço do banco de dados Oracle.\n        :param port: Porta de conexão ao banco.\n        :param sid: SID do banco de dados.\n        :param user: Usuário do banco de dados.\n        :param password: Senha do usuário.\n        \"\"\"\n        password = '!Petiquinho24'\n        self.url = f\"jdbc:oracle:thin:{user}/{password.replace('@', '%40')}@//{host}:{port}/{sid}\"\n\n        self.spark = SparkSession.builder \\\n            .appName(\"OracleClient\") \\\n            .config(\"spark.jars\", \"/Volumes/prd/bdq/denodo/ojdbc7.jar\") \\\n            .getOrCreate()\n\n\n    def table_exists(self, schema, table):\n        \"\"\"\n        Verifica se uma tabela existe no schema informado usando JDBC.\n\n        :param schema: O nome do schema.\n        :param table: O nome da tabela.\n        :return: Retorna True se a tabela existir, caso contrário False.\n        \"\"\"\n        query = f\"\"\"\n            SELECT COUNT(*)\n            FROM all_tables\n            WHERE owner = '{schema.upper()}' AND table_name = '{table.upper()}'\n        \"\"\"\n        result = self.spark.read \\\n            .format(\"jdbc\") \\\n            .option(\"url\", self.url) \\\n            .option(\"query\", query) \\\n            .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n            .option(\"pushDownAggregate\", \"true\") \\\n            .option(\"pushDownPredicate\", \"true\") \\\n            .option(\"fetchSize\", 200) \\\n            .load()\n\n        exists = result.collect()[0][0]\n        return exists > 0\n\n    def fetch_data(self, database, table, fields='*', filters=None, batch_size=100000):\n        from pyspark.sql import DataFrame\n        \n        # Monta a query base\n        base_query = f\"SELECT {fields} FROM {database}.{table}\"\n\n        # Adiciona os filtros, se existirem\n        if filters:\n            base_query += f\" WHERE {filters}\"\n\n        print(\"base_query: \", base_query)\n        \n        # Inicializa variáveis para paginação\n        offset = 0\n        final_df = None  # DataFrame acumulado\n\n        while True:\n            # Adiciona paginação à consulta\n            paginated_query = f\"{base_query} OFFSET {offset} ROWS FETCH NEXT {batch_size} ROWS ONLY\"\n\n            # Lê o lote atual de dados\n            batch_df = self.spark.read \\\n                .format(\"jdbc\") \\\n                .option(\"url\", self.url) \\\n                .option(\"query\", paginated_query) \\\n                .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n                .option(\"pushDownAggregate\", \"true\") \\\n                .option(\"pushDownPredicate\", \"true\") \\\n                .option(\"fetchSize\", 200) \\\n                .load()\n\n            # Verifica se o lote retornou dados\n            if batch_df.rdd.isEmpty():\n                break\n\n            # Faz union com o DataFrame acumulado\n            if final_df is None:\n                final_df = batch_df  # Primeiro lote\n            else:\n                final_df = final_df.union(batch_df)  # União incremental\n\n            # Atualiza o offset\n            offset += batch_size\n\n        # Retorna o DataFrame final ou um DataFrame vazio se nenhum dado for encontrado\n        if final_df is None:\n            return self.spark.createDataFrame([], schema=None)  # DataFrame vazio\n        else:\n            return final_df\n\n    def fetch_data_with_query(self, query):\n\n        df = self.spark.read \\\n            .format(\"jdbc\") \\\n            .option(\"url\", self.url) \\\n            .option(\"query\", query) \\\n            .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n            .option(\"pushDownAggregate\", \"true\") \\\n            .option(\"pushDownPredicate\", \"true\") \\\n            .option(\"fetchSize\", 1000) \\\n            .load()\n\n        # Retorna o DataFrame final ou um DataFrame vazio se nenhum dado for encontrado\n        if df is None:\n            return self.spark.createDataFrame([], schema=None)  # DataFrame vazio\n        else:\n            return df\n\n    def create_table_if_not_exists(self, schema, table, columns):\n        \"\"\"\n        Verifica se a tabela existe e cria a tabela se ela não existir.\n\n        :param schema: O nome do schema.\n        :param table: O nome da tabela.\n        :param columns: Dicionário contendo o nome das colunas e seus tipos (ex: {'id': 'NUMBER', 'nome': 'VARCHAR2(100)'}).\n        \"\"\"\n        if not self.table_exists(schema, table):\n            print(f\"Tabela {schema}.{table} não existe. Criando...\")\n            columns_definitions = \", \".join([f\"{col} {dtype}\" for col, dtype in columns.items()])\n            create_table_query = f\"CREATE TABLE {schema}.{table} ({columns_definitions})\"\n            try:\n                self.spark.read \\\n                    .format(\"jdbc\") \\\n                    .option(\"url\", self.url) \\\n                    .option(\"query\", create_table_query) \\\n                    .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n                    .load()\n                print(f\"Tabela {schema}.{table} criada com sucesso!\")\n            except Exception as e:\n                print(f\"Erro ao criar a tabela: {str(e)}\")\n        else:\n            print(f\"Tabela {schema}.{table} já existe.\")\n\n    def insert_data_into_table(self, df, schema, table):\n        \"\"\"\n        Insere dados de um PySpark DataFrame em uma tabela Oracle.\n        \"\"\"\n        if not self.table_exists(schema, table):\n            raise ValueError(f\"A tabela {schema}.{table} não existe. Crie a tabela primeiro.\")\n\n        df.write \\\n            .format(\"jdbc\") \\\n            .option(\"url\", self.url) \\\n            .option(\"dbtable\", f\"{schema}.{table}\") \\\n            .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n            .mode(\"append\") \\\n            .save()\n\n        print(f\"Dados inseridos com sucesso na tabela {schema}.{table}.\")\n\n    def close(self):\n        \"\"\"Fecha a sessão do Spark.\"\"\"\n        self.spark.stop()\n\n    def help_check(self):\n        \"\"\"Fecha a sessão do Spark.\"\"\"\n        print(\"Running...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}